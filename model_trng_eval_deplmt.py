#Import the required Libraries
import streamlit as st

def app6_page():
    '''
    Wraps as a function to import to create multiple pages
    Args:
        none
    Returns:
        none
    '''
    
    st.markdown("<h1 style='text-align: center; color: grey;'>Model Training, Evaluation & Deployment Insights<br/><br/></h1>", unsafe_allow_html=True)
    st.markdown("<h4>Analysis of Prisoner Count, Incarceration, and Crime Rates through Multiple Linear Regression and Vector Autoregression</h4>", unsafe_allow_html=True)
    
    
    st.markdown("<h5>Model Training</h5>", unsafe_allow_html=True)

    st.write('One of our objectives was to find the correlation between prisoner count, incarceration rate, violent crime rate, and property crime rate. We first calculated the percent change in incarceration, violent crime, and property crime, and this is because we wanted to show these crime rates over time which would be over 15 years. To better understand the correlation between prisoner count and all the other crime rates mentioned above, we should use a Multiple Linear Regression model or a Multivariate Time Series model for our data. We can model the prisoner count as a function of the incarceration, violent, and property crime rates using multiple linear regression. In this case, the prisoner count will become the dependent variable (y-axis), and the incarceration rate, violent crime rate, and property crime rate will be the independent variables (x-axis). This approach will allow us to quantify the relationships between these variables and understand how crime and incarceration rates are associated with changes in prisoner counts.') 

    st.write('Once we specify our model, we use a linear regression package like Scikit-learn in Python to fit our model. Scikit-learn will find the best-fitting line that minimizes the differences between the observed prisoner count and those predicted by the model. On the other hand, multivariate time series models like Vector Autoregression (VAR) can capture the linear relationships between multiple time series variables at various time lags and are suitable for analyzing how different types of crime rates and incarceration rates influence each other over time. This is great for our data since it covers 15 years. Once we specify our model, which is VAR, we will use a stats model in Python to fit the VAR model.')

    st.markdown("<h5>Model Evaluation</h5>", unsafe_allow_html=True)
    st.write('Here is a mathematical representation of a multiple linear regression model:')

    codeOne = '''Prisoner Count = β0 + β1 × Incarceration Rate + β2 × Violent Crime Rate + β3 × Property Crime Rate + ϵ'''
    st.code(codeOne, language='python')
    

    st.write('After fitting the model, one should get estimates for the β figures. These can tell anyone about the relationships between the variables. For instance, β1 can tell us how much the prisoner count is expected to change for a one-unit increase in the incarceration rate, assuming all factors remain unchanged. If there is an increase in the incarceration rate by one percentage, the prisoner count is expected to change by β units. Therefore, if β is positive, it means an increase in the incarceration rate will lead to an increase in the prisoner count. If β is negative, it means an increase in the incarceration rate will lead to a decrease in the prisoner count. Once you understand the relationships between the variables, you will use metrics like RMSE (Root Mean Square Error) and MAE (Mean Absolute Error) to give you a sense of how well the model fits the data. RMSE will measure the differences between the predicted and actual prisoner counts, and MAE will tell you how many prisoners, on average, your models predictions are off by,regardless of whether the predictions are too high or too low. A lower MAE indicates that the models predictions are, on average, closer to the actual values, and hence the model is performing well. A higher MAE suggests that the predictions are, on average, further from the actual values, indicating poorer model performance. Lastly, several key assumptions, such as linearity and independence, must be met when performing linear regression. These assumptions are essential to check because it validates the multiple linear regression model. If one or more assumptions are violated, modifying the model, transforming the variables, or choosing a different modeling approach may be necessary to obtain valid results')

    st.write('When it comes to the VAR model, we can use Impulse Response Functions (IRF) to evaluate our model. Let us break this down a bit. Imagine you have a pond, and you throw a stone into it. The splash is the initial "shock," and the ripples that spread out are the ponds "reaction" to the stone. This is IRF. It is studying those ripples. It is looking at what happens to one thing (like prisoner count) when something else (like violent crime rate) is suddenly changed or "shocked." The "Shock" is the sudden change, like the violent crime rate suddenly increasing, and the "Reaction" is what happens to another variable, like the prisoner count, resulting from that shock. IRF helps us understand how different things are connected, how they react to sudden changes, and what might happen. In the context of this data, IRF will help us understand how a change in one variable, such as violent crime rate, might influence others, such as prisoner count, over time.')

    st.markdown("<h5>Model Deployment</h5>", unsafe_allow_html=True)
    st.write('We are ready to deploy our model at this step of the Data Science Process, but there are a few checkpoints. The checkpoints involve training the model, testing it, and saving it to a file that can be loaded later. We finalize the model by training the model on the full dataset. It is necessary to use all available data to train the final model, ensuring they are tuned and validated. We then save the model to a file using Python libraries such as "pickle" or "joblib".Secondly, we will create a Web Service. Usually, if you are deploying a machine learning model, you will typically wrap it in a web service that allows users or applications to interact with it. We use a web framework like Flask, Django, or FastAPI to create the web service. We also need to ensure we have all the files necessary for it to run, which might include any Python scripts or any HTML/CSS/JavaScript files.')

    st.write('We will now test our web service on a local machine to ensure it works properly. Once everything passes, we will choose our hosting platforms, such as AWS, Azure, or Google Cloud, to host the web application. Once all dependencies, security settings, and the server have been configured on the hosting platform, we can deploy our application or model. After deployment, you will want to monitor your application or model to ensure it works as expected. You might also need to periodically update or retrain your model as new data comes in. Finally, the best practice is to provide clear documentation using the web application with the most accurate and detailed instructions.')

    st.markdown("<h5>Conclusion</h5>", unsafe_allow_html=True)
    st.write('Our dataset focused on prisoner count, incarceration rate, violent crime rate, and property crime rate across various states and years. The goal was to investigate correlations and temporal patterns within these variables to answer specific questions related to crime and incarceration trends. The initial analysis involved understanding the datasets structure, identifying relevant columns, and preparing for modeling. The calculated columns for incarceration, violent, and property crime rates were essential for further analysis. Two main modeling approaches were discussed: Multiple Linear Regression and Vector Autoregression (VAR). Both models were evaluated using appropriate techniques and deployed. Reflecting on the data science process, we were able to effectively collect the necessary data, clean the data, and prepare the data for model training, evaluation, and deployment. We laid out a pathway to uncover insights and make informed predictions that can be valuable for law enforcement agencies, policymakers, and researchers aiming to understand and address complex issues at the intersection of crime, law, and society. Integrating data science into analyzing complex issues (like crime and incarceration) can profoundly change how decisions and policies are crafted. Using quantitative analysis, stakeholders can gain deeper insights, make more informed decisions, and develop more effective policies grounded in practical evidence. It is a testament to the growing importance and influence of data science in various domains of life.')